{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pyvista as pv\n",
    "import torch\n",
    "import vtk\n",
    "import warp as wp\n",
    "\n",
    "if sys.path[0] != \"..\":\n",
    "    sys.path.insert(0, \"..\")\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "torch.cuda.device(device)\n",
    "wp.init()\n",
    "wp.set_device(str(device))\n",
    "\n",
    "from modulus.distributed import DistributedManager\n",
    "\n",
    "DistributedManager.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.environ.get(\"SLURM_JOB_NAME\", None) is None:\n",
    "    dataset_orig_path = Path(\"/data/src/modulus/data/drivaer_aws/\")\n",
    "    dataset_part_path = Path(\"/data/src/modulus/data/drivaer_aws/partitions\")\n",
    "    output_path = dataset_orig_path / f\"inference/seploss4-01/\"\n",
    "    # model_path = Path(\"/data/src/modulus/models/fignet/drivaerml/lrsoc/model_00999.pth\")\n",
    "    model_path = Path(\"/data/src/modulus/models/fignet/drivaerml/seploss4-01/model_00999.pth\")\n",
    "    pc_path = Path(\"/data/src/modulus/data/drivaer_aws/original_pointclouds\")\n",
    "else:\n",
    "    dataset_orig_path = Path(\"/lustre/fsw/portfolios/coreai/projects/coreai_modulus_cae/datasets/drivaer_aws/drivaer_data_full\")\n",
    "    dataset_part_path = Path(\"/lustre/fsw/portfolios/coreai/projects/coreai_modulus_cae/datasets/drivaer_aws/partitions/100_200_400/\")\n",
    "    # output_path = Path(\"/lustre/fsw/portfolios/coreai/users/akamenev/outputs/fignet/drivaerml/inference/new/sharedloss\")\n",
    "    output_path = Path(\"/lustre/fsw/portfolios/coreai/users/akamenev/outputs/fignet/drivaerml/inference/new/seploss\")\n",
    "    # model_path = Path(\"/lustre/fsw/portfolios/coreai/users/akamenev/outputs/fignet/drivaerml/9/model_00999.pth\")\n",
    "    model_path = Path(\"/lustre/fsw/portfolios/coreai/users/akamenev/outputs/fignet/drivaerml/seploss-01/best/model_00999.pth\")\n",
    "    # pc_path = Path(\"/lustre/fsw/portfolios/coreai/projects/coreai_modulus_cae/datasets/drivaer_aws/aero-benchmarking/original_pointclouds/\")\n",
    "    pc_path = Path(\"/lustre/fsw/portfolios/coreai/users/ktangsali/inference_pc_generation/original_pointclouds/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.data\n",
    "\n",
    "\n",
    "num_points = 500_000\n",
    "datamodule = src.data.DrivAerMLDataModule(\n",
    "    data_path=dataset_part_path,\n",
    "    num_points=num_points\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.networks\n",
    "from modulus.models.figconvnet.geometries import GridFeaturesMemoryFormat\n",
    "\n",
    "\n",
    "model = src.networks.FIGConvUNetDrivAerML(\n",
    "  aabb_max=[2.0,  1.8,  2.6],\n",
    "  aabb_min=[-2.0, -1.8, -1.5],\n",
    "  hidden_channels=[16, 16, 16],\n",
    "  in_channels=1,\n",
    "  kernel_size=5,\n",
    "  mlp_channels=[512, 512], #[2048, 2048],\n",
    "  neighbor_search_type=\"radius\",\n",
    "  num_down_blocks=1,\n",
    "  num_levels=2,\n",
    "  out_channels=4,\n",
    "  pooling_layers=[2],\n",
    "  pooling_type=\"max\",\n",
    "  reductions=[\"mean\"],\n",
    "  resolution_memory_format_pairs=[\n",
    "    (GridFeaturesMemoryFormat.b_xc_y_z, [  5, 150, 100]),\n",
    "    (GridFeaturesMemoryFormat.b_yc_x_z, [250,   3, 100]),\n",
    "    (GridFeaturesMemoryFormat.b_zc_x_y, [250, 150,   2]),\n",
    "  ],\n",
    "  use_rel_pos_encode=True,\n",
    ")\n",
    "# Load checkpoint.\n",
    "chk = torch.load(model_path)\n",
    "model.load_state_dict(chk[\"model\"])\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from modulus.datapipes.cae.readers import read_vtp\n",
    "\n",
    "\n",
    "def convert_to_triangular_mesh(\n",
    "    polydata, write=False, output_filename=\"surface_mesh_triangular.vtu\"\n",
    "):\n",
    "    \"\"\"Converts a vtkPolyData object to a triangular mesh.\"\"\"\n",
    "    tet_filter = vtk.vtkDataSetTriangleFilter()\n",
    "    tet_filter.SetInputData(polydata)\n",
    "    tet_filter.Update()\n",
    "\n",
    "    tet_mesh = pv.wrap(tet_filter.GetOutput())\n",
    "\n",
    "    if write:\n",
    "        tet_mesh.save(output_filename)\n",
    "\n",
    "    return tet_mesh\n",
    "\n",
    "\n",
    "def fetch_mesh_vertices(mesh):\n",
    "    \"\"\"Fetches the vertices of a mesh.\"\"\"\n",
    "    points = mesh.GetPoints()\n",
    "    num_points = points.GetNumberOfPoints()\n",
    "    vertices = [points.GetPoint(i) for i in range(num_points)]\n",
    "    return vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.eval_funcs import rrmse\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "@torch.no_grad\n",
    "def run_inference(k: int = 4):\n",
    "    for sample in datamodule.test_dataloader():\n",
    "        vertices_denorm = model.data_dict_to_input(sample)\n",
    "        vertices = datamodule.encode(vertices_denorm, \"coordinates\")\n",
    "        normalized_pred, _ = model(vertices)\n",
    "        normalized_p_pred = normalized_pred[..., :1]\n",
    "        denorm_p_pred = datamodule.decode(normalized_p_pred, \"pressure\")\n",
    "        normalized_wss_pred = normalized_pred[..., 1:]\n",
    "        denorm_wss_pred = datamodule.decode(normalized_wss_pred, \"shear_stress\")\n",
    "\n",
    "        # Read the original surface mesh.\n",
    "        idx = sample[\"design\"][0]\n",
    "        vtp_file = dataset_orig_path / f\"run_{idx}/boundary_{idx}.vtp\"\n",
    "        print(f\"Reading {vtp_file}\")\n",
    "        mesh = pv.read(vtp_file)\n",
    "        mesh = mesh.cell_data_to_point_data()\n",
    "\n",
    "        # Interpolate predictions on GT mesh.\n",
    "        nbrs_surface = NearestNeighbors(\n",
    "            n_neighbors=k, algorithm=\"ball_tree\"\n",
    "        ).fit(vertices_denorm[0].cpu().numpy())\n",
    "\n",
    "        distances, indices = nbrs_surface.kneighbors(mesh.points)\n",
    "        if k == 1:\n",
    "            indices = indices.flatten()\n",
    "            pressure_pred_mesh = denorm_p_pred[0][indices]\n",
    "            shear_stress_pred_mesh = denorm_wss_pred[0][indices]\n",
    "        else:\n",
    "            # distances = distances.astype(np.float32)\n",
    "            # Weighted kNN interpolation\n",
    "            # Avoid division by zero by adding a small epsilon\n",
    "            epsilon = 1e-8\n",
    "            weights = 1 / (distances + epsilon)\n",
    "            weights_sum = np.sum(weights, axis=1, keepdims=True)\n",
    "            normalized_weights = weights / weights_sum\n",
    "            # Fetch the predictions of the k nearest neighbors\n",
    "            pressure_neighbors = denorm_p_pred[0][indices]  # Shape: (n_samples, k, 1)\n",
    "            shear_stress_neighbors = denorm_wss_pred[0][indices]  # Shape: (n_samples, k, 3)\n",
    "\n",
    "            # Compute the weighted average\n",
    "            pressure_pred_mesh = np.sum(normalized_weights[:, :, np.newaxis] * pressure_neighbors.cpu().numpy(), axis=1)\n",
    "            shear_stress_pred_mesh = np.sum(normalized_weights[:, :, np.newaxis] * shear_stress_neighbors.cpu().numpy(), axis=1)\n",
    "\n",
    "            # Convert back to torch tensors\n",
    "            pressure_pred_mesh = torch.from_numpy(pressure_pred_mesh).to(device)\n",
    "            shear_stress_pred_mesh = torch.from_numpy(shear_stress_pred_mesh).to(device)\n",
    "\n",
    "        mesh.point_data[\"pMeanTrimPred\"] = pressure_pred_mesh.cpu().float().numpy()\n",
    "        mesh.point_data[\"wallShearStressMeanTrimPred\"] = shear_stress_pred_mesh.cpu().float().numpy()\n",
    "        out_path = output_path / f\"simmesh/500K_k{k}\"\n",
    "        out_path.mkdir(parents=True, exist_ok=True)\n",
    "        mesh.save(out_path / f\"inference_mesh_{idx}.vtp\")\n",
    "\n",
    "        print(\"Done.\")\n",
    "        print(\n",
    "            rrmse(torch.tensor(mesh.point_data[\"pMeanTrim\"]), torch.tensor(mesh.point_data[\"pMeanTrimPred\"])),\n",
    "            rrmse(torch.tensor(mesh.point_data[\"wallShearStressMeanTrim\"][:, 0]), torch.tensor(mesh.point_data[\"wallShearStressMeanTrimPred\"][:, 0])),\n",
    "            rrmse(torch.tensor(mesh.point_data[\"wallShearStressMeanTrim\"][:, 1]), torch.tensor(mesh.point_data[\"wallShearStressMeanTrimPred\"][:, 1])),\n",
    "            rrmse(torch.tensor(mesh.point_data[\"wallShearStressMeanTrim\"][:, 2]), torch.tensor(mesh.point_data[\"wallShearStressMeanTrimPred\"][:, 2])),\n",
    "        )\n",
    "        torch.cuda.empty_cache()\n",
    "        # break\n",
    "\n",
    "run_inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def run_inference_on_pc(pc_size: int, k: int = 4):\n",
    "    for sample in datamodule.test_dataloader():\n",
    "        vertices_denorm = model.data_dict_to_input(sample)\n",
    "        vertices = datamodule.encode(vertices_denorm, \"coordinates\")\n",
    "        normalized_pred, _ = model(vertices)\n",
    "        normalized_p_pred = normalized_pred[..., :1]\n",
    "        denorm_p_pred = datamodule.decode(normalized_p_pred, \"pressure\")\n",
    "        normalized_wss_pred = normalized_pred[..., 1:]\n",
    "        denorm_wss_pred = datamodule.decode(normalized_wss_pred, \"shear_stress\")\n",
    "\n",
    "        # Read the original surface mesh.\n",
    "        idx = sample[\"design\"][0]\n",
    "        vtp_file = pc_path / f\"input_pc_{pc_size}_run_{idx}_final.vtp\"\n",
    "        print(f\"Reading {vtp_file}\")\n",
    "        mesh = pv.read(vtp_file)\n",
    "\n",
    "        # Interpolate predictions on GT mesh.\n",
    "        nbrs_surface = NearestNeighbors(\n",
    "            n_neighbors=k, algorithm=\"ball_tree\"\n",
    "        ).fit(vertices_denorm[0].cpu().numpy())\n",
    "\n",
    "        distances, indices = nbrs_surface.kneighbors(mesh.points)\n",
    "        if k == 1:\n",
    "            indices = indices.flatten()\n",
    "            pressure_pred_mesh = denorm_p_pred[0][indices]\n",
    "            shear_stress_pred_mesh = denorm_wss_pred[0][indices]\n",
    "        else:\n",
    "            # distances = distances.astype(np.float32)\n",
    "            # Weighted kNN interpolation\n",
    "            # Avoid division by zero by adding a small epsilon\n",
    "            epsilon = 1e-8\n",
    "            weights = 1 / (distances + epsilon)\n",
    "            weights_sum = np.sum(weights, axis=1, keepdims=True)\n",
    "            normalized_weights = weights / weights_sum\n",
    "            # Fetch the predictions of the k nearest neighbors\n",
    "            pressure_neighbors = denorm_p_pred[0][indices]  # Shape: (n_samples, k, 1)\n",
    "            shear_stress_neighbors = denorm_wss_pred[0][indices]  # Shape: (n_samples, k, 3)\n",
    "\n",
    "            # Compute the weighted average\n",
    "            pressure_pred_mesh = np.sum(normalized_weights[:, :, np.newaxis] * pressure_neighbors.cpu().numpy(), axis=1)\n",
    "            shear_stress_pred_mesh = np.sum(normalized_weights[:, :, np.newaxis] * shear_stress_neighbors.cpu().numpy(), axis=1)\n",
    "\n",
    "            # Convert back to torch tensors\n",
    "            pressure_pred_mesh = torch.from_numpy(pressure_pred_mesh).to(device)\n",
    "            shear_stress_pred_mesh = torch.from_numpy(shear_stress_pred_mesh).to(device)\n",
    "\n",
    "        mesh.point_data[\"pMeanTrimPred\"] = pressure_pred_mesh.cpu().float().numpy()\n",
    "        mesh.point_data[\"wallShearStressMeanTrimPred\"] = shear_stress_pred_mesh.cpu().float().numpy()\n",
    "        out_path = output_path / f\"pc/500K_k{k}\"\n",
    "        out_path.mkdir(parents=True, exist_ok=True)\n",
    "        mesh.save(out_path / f\"inference_pc_{pc_size}_{idx}.vtp\")\n",
    "        print(\"Done.\")\n",
    "        torch.cuda.empty_cache()\n",
    "        # break\n",
    "\n",
    "# run_inference_on_pc(5_000_000)\n",
    "# run_inference_on_pc(10_000_000)\n",
    "# run_inference_on_pc(20_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def inference_on_sim_mesh():\n",
    "    for idx in [100, 200, 300, 400, 500][:1]:\n",
    "        mesh_gt = pv.read(dataset_orig_path / f\"run_{idx}/boundary_{idx}.vtp\")\n",
    "        mesh_gt = mesh_gt.cell_data_to_point_data()\n",
    "        step = 500_000 # num_points\n",
    "        p_chunks = []\n",
    "        wss_chunks = []\n",
    "        rng = np.random.default_rng(1)\n",
    "        indices = rng.permutation(range(mesh_gt.number_of_points))\n",
    "        for i_start in range(0, mesh_gt.number_of_points, step):\n",
    "            vertices_denorm = torch.as_tensor(\n",
    "                mesh_gt.points[indices[i_start : i_start + step]], device=device\n",
    "            ).unsqueeze(0)\n",
    "            vertices = datamodule.encode(vertices_denorm, \"coordinates\")\n",
    "            normalized_pred, _ = model(vertices)\n",
    "\n",
    "            normalized_p_pred = normalized_pred[..., :1]\n",
    "            denorm_p_pred = datamodule.decode(normalized_p_pred, \"pressure\")\n",
    "            p_chunks.append(denorm_p_pred.cpu())\n",
    "\n",
    "            normalized_wss_pred = normalized_pred[..., 1:]\n",
    "            denorm_wss_pred = datamodule.decode(normalized_wss_pred, \"shear_stress\")\n",
    "            wss_chunks.append(denorm_wss_pred.cpu())\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        pressure_pred_mesh = torch.cat(p_chunks, dim=1)[0]\n",
    "        shear_stress_pred_mesh = torch.cat(wss_chunks, dim=1)[0]\n",
    "        mesh_gt.point_data[\"pMeanTrimPred\"] = pressure_pred_mesh.cpu().float().numpy()\n",
    "        mesh_gt.point_data[\"wallShearStressMeanTrimPred\"] = shear_stress_pred_mesh.cpu().float().numpy()\n",
    "        mesh_gt.save(output_path / f\"inference_mesh_{idx}.vtp\")\n",
    "        print(\"Done.\")\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# inference_on_sim_mesh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v = read_vtp(str(dataset_orig_path / f\"inference_point_cloud_{idx}.vtp\"))\n",
    "# v = read_vtp(str(dataset_orig_path / f\"run_100/boundary_100.vtp\"))\n",
    "# mesh_gt = pv.read(dataset_orig_path / f\"run_100/boundary_100.vtp\")\n",
    "\n",
    "# mesh_gt = mesh_gt.cell_data_to_point_data()\n",
    "# mesh_pred = pv.read(dataset_orig_path / f\"inference/inference_mesh_100.vtp\")\n",
    "# rrmse(torch.tensor(mesh_pred.point_data[\"pMeanTrim\"]), torch.tensor(mesh_pred.point_data[\"pMeanTrimPred\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
